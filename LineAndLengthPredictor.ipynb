{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMM0KIa0T8sWXyJlNvqORxI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohithreddy878/tcdzMachineLearning/blob/main/LineAndLengthPredictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 1: Install required libraries"
      ],
      "metadata": {
        "id": "VqZYtNQRVTja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IPtTuJ-RDtxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 2: Import libraries"
      ],
      "metadata": {
        "id": "R6Qj5KGXVZjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from datasets import Dataset\n",
        "from transformers import MobileBertTokenizer, MobileBertForSequenceClassification, Trainer, TrainingArguments, MobileBertModel, default_data_collator\n",
        "import torch.nn as nn\n"
      ],
      "metadata": {
        "id": "-PUoVSo8VjAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 3: Load your dataset"
      ],
      "metadata": {
        "id": "Ptfo0W1iVmWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Pawz8GimaIEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "u0VIIDndZg6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your own file path or mount Google Drive to load your excel file\n",
        "df = pd.read_excel(\"AllDeliveries2023.xlsx\", engine=\"openpyxl\")\n",
        "df.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LrkjgDVdaOza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values\n",
        "df['length'] = df['length'].fillna('None')\n",
        "df['line'] = df['line'].fillna('None')"
      ],
      "metadata": {
        "id": "euVWy30lVpxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 4: Train-test split"
      ],
      "metadata": {
        "id": "NgMAr7DfVtWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Split the data into train and validation sets\n",
        "train_data, eval_data = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "length_encoder = LabelEncoder()\n",
        "line_encoder = LabelEncoder()\n",
        "\n",
        "# Fit on both training and evaluation data for length\n",
        "length_encoder.fit(pd.concat([train_data['length'], eval_data['length']]))\n",
        "# Similarly for line\n",
        "line_encoder.fit(pd.concat([train_data['line'], eval_data['line']]))\n",
        "\n",
        "train_data['length'] = length_encoder.transform(train_data['length'])\n",
        "train_data['line'] = line_encoder.transform(train_data['line'])\n",
        "\n",
        "# Similarly for eval_data\n",
        "eval_data['length'] = length_encoder.transform(eval_data['length'])\n",
        "eval_data['line'] = line_encoder.transform(eval_data['line'])\n"
      ],
      "metadata": {
        "id": "MGwUx5GaVxA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert to Hugging Face datasets\n",
        "train_dataset = Dataset.from_pandas(train_data)\n",
        "eval_dataset = Dataset.from_pandas(eval_data)"
      ],
      "metadata": {
        "id": "y8ak01FoFjiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 5: Tokenization and Encoding"
      ],
      "metadata": {
        "id": "Tsv3PpDfVzhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MobileBertTokenizer\n",
        "\n",
        "# Load the MobileBERT tokenizer\n",
        "tokenizer = MobileBertTokenizer.from_pretrained(\"google/mobilebert-uncased\")\n",
        "\n",
        "# Tokenize function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['cb_comm'], padding='max_length', truncation=True, max_length=512)\n",
        "\n",
        "# Apply tokenization to the datasets\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Ensure the format of the datasets includes the tokenized inputs and labels\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'length', 'line'])\n",
        "eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'length', 'line'])\n"
      ],
      "metadata": {
        "id": "JhxXAl-CV3r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 6: Create Dual-Head Model"
      ],
      "metadata": {
        "id": "fqbqFctlV5an"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DualHeadMobileBertForMultiLabelClassification(MobileBertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.classifier_length = nn.Linear(config.hidden_size, 1)  # Regression for length\n",
        "        self.classifier_line = nn.Linear(config.hidden_size, 1)    # Regression for line\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels_length=None, labels_line=None):\n",
        "        # Forward pass through MobileBERT\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs[0]  # The last hidden state (sequence of hidden states)\n",
        "\n",
        "        # Apply separate classifiers for length and line\n",
        "        logits_length = self.classifier_length(sequence_output[:, 0, :])  # Length classifier\n",
        "        logits_line = self.classifier_line(sequence_output[:, 0, :])    # Line classifier\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = None\n",
        "        if labels_length is not None and labels_line is not None:\n",
        "            # Using mean squared error loss for regression tasks\n",
        "            loss_fct = nn.MSELoss()\n",
        "            loss_length = loss_fct(logits_length.view(-1), labels_length)\n",
        "            loss_line = loss_fct(logits_line.view(-1), labels_line)\n",
        "            loss = (loss_length + loss_line) / 2  # Average the two losses\n",
        "\n",
        "        # Return loss and logits\n",
        "        return (loss, logits_length, logits_line) if loss is not None else (logits_length, logits_line)\n"
      ],
      "metadata": {
        "id": "ws4LXNQ3V8J4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 7: Model and training args"
      ],
      "metadata": {
        "id": "sTIipCPBV-pC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",            # Output directory for model and logs\n",
        "    eval_strategy=\"epoch\",       # Evaluate every epoch\n",
        "    save_strategy=\"epoch\",             # Save model every epoch\n",
        "    per_device_train_batch_size=16,    # Batch size for training\n",
        "    per_device_eval_batch_size=16,     # Batch size for evaluation\n",
        "    num_train_epochs=3,                # Number of training epochs\n",
        "    learning_rate=2e-5,                # Learning rate\n",
        "    weight_decay=0.01,                 # Weight decay\n",
        "    logging_dir=\"./logs\",              # Directory for logs\n",
        "    logging_steps=100,                 # Log every 100 steps\n",
        "    load_best_model_at_end=True,       # Load best model at the end of training\n",
        "    metric_for_best_model=\"f1_micro_avg\",  # Metric to determine best model\n",
        ")\n"
      ],
      "metadata": {
        "id": "b1NTOi4YhVMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 8: Define metrics (optional but helpful)"
      ],
      "metadata": {
        "id": "xv7cVzd7WDpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    logits_length, logits_line = logits  # From the model output\n",
        "    preds_length = np.argmax(logits_length, axis=-1)\n",
        "    preds_line = np.argmax(logits_line, axis=-1)\n",
        "\n",
        "    labels = np.array(labels)\n",
        "    labels_length, labels_line = labels[:, 0], labels[:, 1]\n",
        "\n",
        "    accuracy_length = accuracy_score(labels_length, preds_length)\n",
        "    accuracy_line = accuracy_score(labels_line, preds_line)\n",
        "    f1_length = f1_score(labels_length, preds_length, average=\"micro\")\n",
        "    f1_line = f1_score(labels_line, preds_line, average=\"micro\")\n",
        "\n",
        "    return {\n",
        "        \"accuracy_length\": accuracy_length,\n",
        "        \"accuracy_line\": accuracy_line,\n",
        "        \"f1_micro_length\": f1_length,\n",
        "        \"f1_micro_line\": f1_line,\n",
        "        \"f1_micro_avg\": (f1_length + f1_line) / 2\n",
        "    }\n"
      ],
      "metadata": {
        "id": "gaoKBFxpWFiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 9:  Initialize Trainer and Start Training"
      ],
      "metadata": {
        "id": "P-0pT1-qWJbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DualHeadMobileBert.from_pretrained(\n",
        "    \"google/mobilebert-uncased\",\n",
        "    num_labels_length=18,  # Number of classes for length\n",
        "    num_labels_line=10     # Number of classes for line\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=default_data_collator,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "nOg3Z03YkcIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 10: Evaluation"
      ],
      "metadata": {
        "id": "hkGBurszWNHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "results = trainer.evaluate(eval_dataset)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy (Length): {results['eval_accuracy_length']}\")\n",
        "print(f\"Accuracy (Line): {results['eval_accuracy_line']}\")\n",
        "print(f\"F1 (Length): {results['eval_f1_micro_length']}\")\n",
        "print(f\"F1 (Line): {results['eval_f1_micro_line']}\")\n",
        "print(f\"Average F1: {results['eval_f1_micro_avg']}\")"
      ],
      "metadata": {
        "id": "kvHYOHKmWPZZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}